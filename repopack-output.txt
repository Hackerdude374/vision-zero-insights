================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2025-06-18T14:00:37.312Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
backend/.repopackignore
backend/app.py
backend/crash_data.sql
backend/db.py
backend/Dockerfile
backend/dockerinstructions.txt
backend/generate_opendata_dataset.py
backend/geo_utils.py
backend/model.py
backend/Procfile
backend/requirements.txt
backend/test_predict.py
LICENSE
notes.txt
README.md

================================================================
Repository Files
================================================================

================
File: backend/.repopackignore
================
crash_data_cleaned.csv

================
File: backend/app.py
================
from flask import Flask, jsonify, request
from db import get_recent_crashes
from model import predict_severity
from flask_cors import CORS
from geo_utils import make_geodf_from_crash_data, filter_by_bbox_coords

from werkzeug.exceptions import Forbidden
import csv


app = Flask(__name__)
CORS(app)  # Enable CORS for all routes
@app.route("/api/crashes")
def crashes():
    borough = request.args.get("borough")
    bbox = request.args.get("bbox")

    data = get_recent_crashes(limit=1000)
    df = pd.DataFrame(data)

    if borough:
        df = filter_by_borough(df, borough)

    if bbox:
        gdf = make_geodf_from_crash_data(df)
        gdf = filter_by_bbox_coords(gdf, bbox)
        df = pd.DataFrame(gdf.drop(columns="geometry"))

    return jsonify(df.to_dict(orient="records"))


@app.route("/api/predict", methods=["POST"])
def predict():
    input_data = request.get_json()
    prediction = predict_severity(input_data)
    return jsonify({"predicted_severity": prediction})

@app.route("/api/admin/upload", methods=["POST"])
def admin_upload():
    # For security: simple token check
    token = request.args.get("token")
    if token != os.getenv("ADMIN_TOKEN"):
        raise Forbidden("Invalid token")

    try:
        with open("crash_data_cleaned.csv", "r") as f:
            reader = csv.DictReader(f)
            conn = psycopg2.connect(DATABASE_URL)
            cur = conn.cursor()
            for row in reader:
                cur.execute("""
                    INSERT INTO crash_data (
                        crash_date, borough, latitude, longitude,
                        number_of_persons_injured, contributing_factor_vehicle_1
                    ) VALUES (%s, %s, %s, %s, %s, %s)
                """, (
                    row["crash_date"],
                    row["borough"],
                    float(row["latitude"]),
                    float(row["longitude"]),
                    int(row["number_of_persons_injured"]),
                    row["contributing_factor_vehicle_1"]
                ))
            conn.commit()
            cur.close()
            conn.close()
            return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(debug=True)

================
File: backend/crash_data.sql
================
CREATE TABLE crash_data (
    crash_date DATE,
    borough TEXT,
    latitude DOUBLE PRECISION,
    longitude DOUBLE PRECISION,
    number_of_persons_injured INTEGER,
    contributing_factor_vehicle_1 TEXT
);

================
File: backend/db.py
================
# backend/db.py
import psycopg2
import os

DATABASE_URL = os.environ.get("DATABASE_URL")

def get_recent_crashes(limit=100):
    query = """
        SELECT crash_date, borough, latitude, longitude,
               number_of_persons_injured, contributing_factor_vehicle_1
        FROM crash_data
        WHERE latitude IS NOT NULL AND longitude IS NOT NULL
        ORDER BY crash_date DESC
        LIMIT %s;
    """
    try:
        conn = psycopg2.connect(DATABASE_URL)
        cur = conn.cursor()
        cur.execute(query, (limit,))
        rows = cur.fetchall()
        columns = [desc[0] for desc in cur.description]
        cur.close()
        conn.close()
        return [dict(zip(columns, row)) for row in rows]
    except Exception as e:
        print("DB error:", e)
        return []

================
File: backend/Dockerfile
================
# Use an official lightweight Python image
FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Set work directory
WORKDIR /app

# Install dependencies
COPY requirements.txt /app/
RUN pip install --upgrade pip && pip install -r requirements.txt

# Copy project files
COPY . /app/

# Expose port (optional for local use)
EXPOSE 5000

# Run the app using gunicorn
CMD ["gunicorn", "app:app", "--bind", "0.0.0.0:5000"]

================
File: backend/dockerinstructions.txt
================
# In the backend folder:
docker build -t visionzero-backend .

docker run -d -p 5000:5000 --name visionzero-api \
  -e DATABASE_URL="your-db-url" \
  visionzero-backend

================
File: backend/generate_opendata_dataset.py
================
import pandas as pd

url = "https://data.cityofnewyork.us/resource/h9gi-nx95.csv?$limit=1000"
df = pd.read_csv(url)

df = df[[
    'crash_date', 'borough', 'latitude', 'longitude',
    'number_of_persons_injured', 'contributing_factor_vehicle_1'
]]

df = df.dropna(subset=['latitude', 'longitude'])
df.to_csv('crash_data_cleaned.csv', index=False)

================
File: backend/geo_utils.py
================
# geo_utils.py

from shapely.geometry import Point
import geopandas as gpd
import pandas as pd

# Convert lat/lon into a GeoDataFrame with Point geometry
def make_geodf_from_crash_data(df, lat_col='latitude', lon_col='longitude'):
    df = df.dropna(subset=[lat_col, lon_col])
    gdf = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),
        crs='EPSG:4326'
    )
    return gdf

# Filter a GeoDataFrame to crashes within a bounding box
def filter_crashes_in_bbox(gdf, min_lon, min_lat, max_lon, max_lat):
    bbox = gpd.GeoSeries([Point(min_lon, min_lat), Point(max_lon, max_lat)]).total_bounds
    return gdf.cx[min_lon:max_lon, min_lat:max_lat]

# Filter to crashes within a specific borough (assumes borough column exists)
def filter_by_borough(df, borough_name):
    return df[df['borough'].str.upper() == borough_name.upper()]

#filter by borough coords
def filter_by_bbox_coords(gdf, bbox_str):
    try:
        min_lon, min_lat, max_lon, max_lat = map(float, bbox_str.split(","))
        return gdf.cx[min_lon:max_lon, min_lat:max_lat]
    except:
        return gdf

================
File: backend/model.py
================
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import joblib
import os
model_file = "model.pkl"
# In-memory ML model setup
model = None
features = ['borough', 'contributing_factor']
label = 'injury_severity'

# Manual label engineering from injury count
def load_and_train_model():
    global model
    try:
        if os.path.exists(model_file):
            model = joblib.load(model_file)
            print("✅ Loaded pre-trained model")
            return
        # Load from DB or CSV
        df = pd.read_csv('crash_data_cleaned.csv')

        # Preprocess
        df['injury_severity'] = df['number_of_persons_injured'].apply(lambda x: 'low' if x == 0 else 'high')
        df = df.dropna(subset=['borough', 'contributing_factor_vehicle_1'])

        df = df.rename(columns={'contributing_factor_vehicle_1': 'contributing_factor'})

        # Encode categoricals
        df = pd.get_dummies(df, columns=features)

        X = df.drop(['injury_severity', 'crash_date', 'latitude', 'longitude', 'number_of_persons_injured'], axis=1)
        y = df['injury_severity']

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        model = RandomForestClassifier()
        model.fit(X_train, y_train)
        
        # Save for reuse
        joblib.dump(model, model_file)
        print("✅ Model trained")
    except Exception as e:
        print("❌ Model training failed:", e)

# Run once at startup
load_and_train_model()

# Predict from input
def predict_severity(data):
    global model
    try:
        input_df = pd.DataFrame([data])
        input_df = pd.get_dummies(input_df)

        # Align with training columns
        model_features = model.feature_names_in_
        for col in model_features:
            if col not in input_df.columns:
                input_df[col] = 0

        input_df = input_df[model_features]
        prediction = model.predict(input_df)[0]
        return prediction
    except Exception as e:
        print("❌ Prediction failed:", e)
        return "unknown"

================
File: backend/Procfile
================
web: gunicorn app:app

================
File: backend/requirements.txt
================
Flask
psycopg2
pandas
geopandas
scikit-learn
gunicorn
flask-cors

================
File: backend/test_predict.py
================
import requests

url = "https://vision-zero-insights.onrender.com/api/predict"
data = {
    "borough": "QUEENS",
    "contributing_factor": "Driver Inattention/Distraction"
}

response = requests.post(url, json=data)
print("Prediction:", response.json())

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Robert 

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: notes.txt
================
// vision_zero_dashboard

// Step 1: Folder Structure

// Root
vision-zero-dashboard/
├── backend/              // Flask API + ML + DB handlers
│   ├── app.py            // Main Flask app
│   ├── db.py             // PostgreSQL connection + queries
│   ├── model.py          // ML model + predictor function
│   ├── geo_utils.py      // GeoPandas/PostGIS helpers
│   └── requirements.txt  // Flask, psycopg2, pandas, scikit-learn
│
├── frontend/             // React + Vite + Tailwind + Mapbox
│   ├── public/
│   ├── src/
│   │   ├── App.jsx
│   │   ├── main.jsx
│   │   ├── components/   // Charts, Maps, Filters
│   │   └── services/     // Axios API calls
│   └── vite.config.js
│
├── deploy/
│   ├── aws_lambda_stop_ec2.py  // Optional: Auto shutdown Lambda
│   └── README_DEPLOY.md        // Deployment notes for AWS (EC2 + S3)
│
└── README.md            // Project overview, setup, purpose

================
File: README.md
================
# vision-zero-insights
Interactive crash analytics dashboard using NYC Vision Zero data with Flask, React, Machine Learning, and PostGIS, deployed on AWS
